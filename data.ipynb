{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import subprocess\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"streams/random_stream-n50000-N100000-a4.txt\"\n",
    "#paths = [file_name for file_name in os.listdir(\"datasets\") if file_name.endswith('.txt')]\n",
    "\n",
    "#param = 64\n",
    "params = [16, 32, 64, 128, 256, 512]\n",
    "\n",
    "num_executions = 30000\n",
    "\n",
    "results = {param: {\"REC\": [], \"HLL\": [], \"PCSA\": []} for param in params}\n",
    "#results = {path: {\"REC\": [], \"HLL\": [], \"PCSA\": []} for path in paths}\n",
    "\n",
    "\n",
    "for param in params:\n",
    "#for path in paths:\n",
    "    for seed in range(1, num_executions + 1):\n",
    "\n",
    "        p = subprocess.run([\"./Cardinality\", \"-f\", path, \"-p\", str(param), \"--seed\", str(seed)], capture_output=True, text=True)\n",
    "\n",
    "        if p.returncode == 0:\n",
    "            for line in p.stdout.splitlines():\n",
    "                if line.startswith(\"Recordinality\"):\n",
    "                    results[param][\"REC\"].append(float(line.split(\": \")[1])) # results[param][\"REC\"]\n",
    "                elif line.startswith(\"HyperLogLog\"):\n",
    "                    results[param][\"HLL\"].append(float(line.split(\": \")[1])) # results[param][\"HLL\"]\n",
    "                elif line.startswith(\"Probabilistic Counting\"):\n",
    "                    results[param][\"PCSA\"].append(float(line.split(\": \")[1])) # results[param][\"PCSA\"]\n",
    "        else:\n",
    "            print(f\"Error with seed {seed}, path {path} and param {param}: {p.stderr}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latex Table for comparison of Real vs Estimation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "averages = {}\n",
    "for path in paths:\n",
    "    averages[path] = {\n",
    "        algo: np.mean(scores) for algo, scores in results[path].items()\n",
    "    }\n",
    "\n",
    "latex_output = \"\"\n",
    "for path, scores in averages.items():\n",
    "    with open(\"datasets/\" + path.split('.')[0] + \".dat\", 'r') as file:\n",
    "        lines = file.readlines()\n",
    "        line_count = len(lines)\n",
    "\n",
    "        row = f\"\\\\textbf{{{path.split('.')[0]}}} & \\\\textbf{{{line_count}}}\"\n",
    "        for algo, avg in scores.items():\n",
    "            # error = np.std(results[path][algo], ddof=0) / line_count\n",
    "            accuracy = 100 - (abs(avg - line_count) / line_count)*100\n",
    "            row += f\" & {avg:.0f} & {accuracy:.1f}\\%\"\n",
    "        latex_output += row + \" \\\\\\\\ \\n\"\n",
    "\n",
    "print(latex_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latex Table for comparison parameters datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "averages = {}\n",
    "for param in params:\n",
    "    averages[param] = {\n",
    "        algo: np.mean(scores) for algo, scores in results[param].items()\n",
    "    }\n",
    "\n",
    "latex_output = \"\"\n",
    "with open(\"datasets/\" + path.split('.')[0] + \".dat\", 'r') as file:\n",
    "    lines = file.readlines()\n",
    "    line_count = len(lines)\n",
    "    latex_output = \"\"\n",
    "\n",
    "    for param, scores in averages.items():\n",
    "        row = f\"\\\\textbf{{{param}}}\"\n",
    "        for algo, avg in scores.items():\n",
    "            error = np.std(results[param][algo], ddof=0) / line_count\n",
    "            # accuracy = 1 - (abs(avg - line_count) / line_count)\n",
    "            row += f\" & {avg:.0f} & {error:.2f}\"\n",
    "        latex_output += row + \" \\\\\\\\ \\n\"\n",
    "\n",
    "print(latex_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latex Table for comparison parameters random streams\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "averages = {}\n",
    "for param in params:\n",
    "    averages[param] = {\n",
    "        algo: np.mean(scores) for algo, scores in results[param].items()\n",
    "    }\n",
    "\n",
    "\n",
    "latex_output = \"\"\n",
    "\n",
    "pattern = r'n(\\d+)-N(\\d+)-a(\\d+)\\.txt'\n",
    "match = re.search(pattern, path)\n",
    "if match:\n",
    "    n = int(match.group(1))\n",
    "    N = int(match.group(2))\n",
    "    alpha = int(match.group(3))\n",
    "\n",
    "    for param, scores in averages.items():\n",
    "        row = f\"\\\\textbf{{{param}}}\"\n",
    "        for algo, avg in scores.items():\n",
    "            error = np.std(results[param][algo], ddof=0) / n\n",
    "            # accuracy = 1 - (abs(avg - line_count) / line_count)\n",
    "            row += f\" & {avg:.0f} & {error:.2f}\"\n",
    "        latex_output += row + \" \\\\\\\\ \\n\"\n",
    "\n",
    "print(latex_output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
